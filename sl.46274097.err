
=====================================================================
This module is only for creating or activating Python environments:
$ mamba create -n myenv -c conda-forge python=3 <package_name>
$ source activate myenv

Only run "pip install" after activating an environment.
Running pip without activating an environment is known to cause issues.

To list available environments:
$ mamba info --envs

Other uses are not tested. More info: https://links.asu.edu/solpy
=====================================================================

  
INFO:.prody:ProDy is configured: verbosity='none'
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltzdesign/boltzdesign_utils.py:637: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return grad * torch.sqrt(torch.tensor(eff_L)) / (gn + 1e-7)
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 12.18it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 18.41it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  6.83it/s]100%|██████████| 1/1 [00:00<00:00,  6.82it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 16.44it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 18.54it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 20.64it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 18.48it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 18.99it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltzdesign/boltzdesign_utils.py:637: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return grad * torch.sqrt(torch.tensor(eff_L)) / (gn + 1e-7)
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 12.18it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 897, in _predict_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 957, in _run
    self.strategy.setup(self)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 154, in setup
    self.model_to_device()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py", line 79, in model_to_device
    self.model.to(self.root_device)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 55, in to
    return super().to(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 8.62 MiB is free. Process 1368336 has 77.98 GiB memory in use. Including non-PyTorch memory, this process has 1.25 GiB memory in use. Of the allocated memory 803.02 MiB is allocated by PyTorch, and 60.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/bin/boltz", line 7, in <module>
    sys.exit(cli())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltz/src/boltz/main.py", line 693, in predict
    trainer.predict(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 858, in predict
    return call._call_and_handle_interrupt(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1004, in _teardown
    self.strategy.teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 535, in teardown
    self.lightning_module.cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in cpu
    return self._apply(lambda t: t.cpu())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torchmetrics/metric.py", line 907, in _apply
    _dummy_tensor = fn(torch.zeros(1, device=self.device))
torch.AcceleratorError: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  9.41it/s]100%|██████████| 1/1 [00:00<00:00,  9.39it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 897, in _predict_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 957, in _run
    self.strategy.setup(self)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 154, in setup
    self.model_to_device()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py", line 79, in model_to_device
    self.model.to(self.root_device)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 55, in to
    return super().to(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 8.62 MiB is free. Process 1368336 has 77.98 GiB memory in use. Including non-PyTorch memory, this process has 1.25 GiB memory in use. Of the allocated memory 803.02 MiB is allocated by PyTorch, and 60.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/bin/boltz", line 7, in <module>
    sys.exit(cli())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltz/src/boltz/main.py", line 693, in predict
    trainer.predict(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 858, in predict
    return call._call_and_handle_interrupt(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1004, in _teardown
    self.strategy.teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 535, in teardown
    self.lightning_module.cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in cpu
    return self._apply(lambda t: t.cpu())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torchmetrics/metric.py", line 907, in _apply
    _dummy_tensor = fn(torch.zeros(1, device=self.device))
torch.AcceleratorError: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.05it/s]100%|██████████| 1/1 [00:00<00:00,  3.05it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 897, in _predict_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 957, in _run
    self.strategy.setup(self)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 154, in setup
    self.model_to_device()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py", line 79, in model_to_device
    self.model.to(self.root_device)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 55, in to
    return super().to(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 8.62 MiB is free. Process 1368336 has 77.98 GiB memory in use. Including non-PyTorch memory, this process has 1.25 GiB memory in use. Of the allocated memory 803.02 MiB is allocated by PyTorch, and 60.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/bin/boltz", line 7, in <module>
    sys.exit(cli())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltz/src/boltz/main.py", line 693, in predict
    trainer.predict(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 858, in predict
    return call._call_and_handle_interrupt(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1004, in _teardown
    self.strategy.teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 535, in teardown
    self.lightning_module.cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in cpu
    return self._apply(lambda t: t.cpu())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torchmetrics/metric.py", line 907, in _apply
    _dummy_tensor = fn(torch.zeros(1, device=self.device))
torch.AcceleratorError: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 17.78it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 897, in _predict_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 957, in _run
    self.strategy.setup(self)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 154, in setup
    self.model_to_device()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py", line 79, in model_to_device
    self.model.to(self.root_device)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 55, in to
    return super().to(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 8.62 MiB is free. Process 1368336 has 77.98 GiB memory in use. Including non-PyTorch memory, this process has 1.25 GiB memory in use. Of the allocated memory 803.02 MiB is allocated by PyTorch, and 60.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/bin/boltz", line 7, in <module>
    sys.exit(cli())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltz/src/boltz/main.py", line 693, in predict
    trainer.predict(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 858, in predict
    return call._call_and_handle_interrupt(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1004, in _teardown
    self.strategy.teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 535, in teardown
    self.lightning_module.cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in cpu
    return self._apply(lambda t: t.cpu())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torchmetrics/metric.py", line 907, in _apply
    _dummy_tensor = fn(torch.zeros(1, device=self.device))
torch.AcceleratorError: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  5.88it/s]100%|██████████| 1/1 [00:00<00:00,  5.87it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 897, in _predict_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 957, in _run
    self.strategy.setup(self)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 154, in setup
    self.model_to_device()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py", line 79, in model_to_device
    self.model.to(self.root_device)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 55, in to
    return super().to(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 8.62 MiB is free. Process 1368336 has 77.98 GiB memory in use. Including non-PyTorch memory, this process has 1.25 GiB memory in use. Of the allocated memory 803.02 MiB is allocated by PyTorch, and 60.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/bin/boltz", line 7, in <module>
    sys.exit(cli())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltz/src/boltz/main.py", line 693, in predict
    trainer.predict(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 858, in predict
    return call._call_and_handle_interrupt(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1004, in _teardown
    self.strategy.teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 535, in teardown
    self.lightning_module.cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in cpu
    return self._apply(lambda t: t.cpu())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torchmetrics/metric.py", line 907, in _apply
    _dummy_tensor = fn(torch.zeros(1, device=self.device))
torch.AcceleratorError: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 13.09it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 897, in _predict_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 957, in _run
    self.strategy.setup(self)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 154, in setup
    self.model_to_device()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py", line 79, in model_to_device
    self.model.to(self.root_device)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 55, in to
    return super().to(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 8.62 MiB is free. Process 1368336 has 77.98 GiB memory in use. Including non-PyTorch memory, this process has 1.25 GiB memory in use. Of the allocated memory 803.02 MiB is allocated by PyTorch, and 60.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/bin/boltz", line 7, in <module>
    sys.exit(cli())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltz/src/boltz/main.py", line 693, in predict
    trainer.predict(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 858, in predict
    return call._call_and_handle_interrupt(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1004, in _teardown
    self.strategy.teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 535, in teardown
    self.lightning_module.cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in cpu
    return self._apply(lambda t: t.cpu())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torchmetrics/metric.py", line 907, in _apply
    _dummy_tensor = fn(torch.zeros(1, device=self.device))
torch.AcceleratorError: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 12.82it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 897, in _predict_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 957, in _run
    self.strategy.setup(self)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 154, in setup
    self.model_to_device()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py", line 79, in model_to_device
    self.model.to(self.root_device)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 55, in to
    return super().to(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 8.62 MiB is free. Process 1368336 has 77.98 GiB memory in use. Including non-PyTorch memory, this process has 1.25 GiB memory in use. Of the allocated memory 803.02 MiB is allocated by PyTorch, and 60.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/bin/boltz", line 7, in <module>
    sys.exit(cli())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltz/src/boltz/main.py", line 693, in predict
    trainer.predict(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 858, in predict
    return call._call_and_handle_interrupt(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1004, in _teardown
    self.strategy.teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 535, in teardown
    self.lightning_module.cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in cpu
    return self._apply(lambda t: t.cpu())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torchmetrics/metric.py", line 907, in _apply
    _dummy_tensor = fn(torch.zeros(1, device=self.device))
torch.AcceleratorError: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 13.83it/s]
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.10 /home/phjiang/.conda/envs/boltz_design/bin/boltz ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 897, in _predict_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 957, in _run
    self.strategy.setup(self)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 154, in setup
    self.model_to_device()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/single_device.py", line 79, in model_to_device
    self.model.to(self.root_device)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 55, in to
    return super().to(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 8.62 MiB is free. Process 1368336 has 77.98 GiB memory in use. Including non-PyTorch memory, this process has 1.25 GiB memory in use. Of the allocated memory 803.02 MiB is allocated by PyTorch, and 60.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/phjiang/.conda/envs/boltz_design/bin/boltz", line 7, in <module>
    sys.exit(cli())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltz/src/boltz/main.py", line 693, in predict
    trainer.predict(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 858, in predict
    return call._call_and_handle_interrupt(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1004, in _teardown
    self.strategy.teardown()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 535, in teardown
    self.lightning_module.cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in cpu
    return self._apply(lambda t: t.cpu())
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torchmetrics/metric.py", line 907, in _apply
    _dummy_tensor = fn(torch.zeros(1, device=self.device))
torch.AcceleratorError: CUDA error: out of memory
Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltzdesign.py", line 829, in <module>
    main()
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltzdesign.py", line 823, in main
    results = run_pipeline_steps(args, config, boltz_model, yaml_dir, output_dir)
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltzdesign.py", line 764, in run_pipeline_steps
    run_boltz_design_step(args, config, boltz_model, yaml_dir,
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltzdesign.py", line 380, in run_boltz_design_step
    run_boltz_design(
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltzdesign/boltzdesign_utils.py", line 1208, in run_boltz_design
    input_res_type, plots, loss_history, distogram_history, sequence_history, traj_coords_list, traj_plddt_list = boltz_hallucination(
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltzdesign/boltzdesign_utils.py", line 936, in boltz_hallucination
    best_batch, plots, loss_history, i_con_loss_history, con_loss_history,plddt_loss_history, distogram_history, sequence_history, traj_coords_list, traj_plddt_list = design(batches, iters=pre_iteration, soft=1.0, mask=mask, chain_masks=chain_masks, learning_rate=learning_rate_pre, length=length, plots=plots, loss_history=loss_history, i_con_loss_history=i_con_loss_history, con_loss_history=con_loss_history, plddt_loss_history=plddt_loss_history, distogram_history=distogram_history, sequence_history=sequence_history, pre_run=pre_run, mask_ligand=mask_ligand, distogram_only=distogram_only, predict_args=predict_args, loss_scales=loss_scales, binder_chain=binder_chain, increasing_contact_over_itr=increasing_contact_over_itr, optimize_contact_per_binder_pos=optimize_contact_per_binder_pos, non_protein_target=non_protein_target, inter_chain_cutoff=inter_chain_cutoff, intra_chain_cutoff=intra_chain_cutoff, num_inter_contacts=num_inter_contacts, num_intra_contacts=num_intra_contacts, save_trajectory=save_trajectory)
  File "/scratch/phjiang/abhi_scratc/setup/BoltzDesign1/boltzdesign/boltzdesign_utils.py", line 920, in design
    total_loss.backward()
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
    return user_fn(self, *args)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/fairscale/nn/checkpoint/checkpoint_activations.py", line 349, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/phjiang/.conda/envs/boltz_design/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 998.69 MiB is free. Including non-PyTorch memory, this process has 78.27 GiB memory in use. Of the allocated memory 73.65 GiB is allocated by PyTorch, and 4.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
